{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4V4RT9cqe85"
   },
   "source": [
    "Copyright 2021 DeepMind Technologies Limited\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9fbViWHqiuH"
   },
   "source": [
    "# Video compression\n",
    "Simplified demo showing how to:\n",
    "1. Load a compressed network and apply it to a video\n",
    "2. Load an augmentation and apply it to a video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQqs7_sXq9D0"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NAQP4Ao1qXxv"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'compressed_vision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     11\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcompressed_vision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m equivariant_networks\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcompressed_vision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m encoder_decoder_unet\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcompressed_vision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m checkpoint_loader\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'compressed_vision'"
     ]
    }
   ],
   "source": [
    "# @title Imports\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from IPython.display import Image as display_image\n",
    "from PIL import Image\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('../../'))\n",
    "\n",
    "from compressed_vision.models import equivariant_networks\n",
    "from compressed_vision.models import encoder_decoder_unet\n",
    "from compressed_vision.utils import checkpoint_loader\n",
    "from compressed_vision.utils import data_utils\n",
    "from compressed_vision.utils import metric_utils\n",
    "from compressed_vision.utils import video_utils\n",
    "\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vqUrrobssVF"
   },
   "source": [
    "# Run compression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FmSUKmV-sqHg"
   },
   "outputs": [],
   "source": [
    "# @title Load models.\n",
    "# @markdown Investigate different augmentations or compression.\n",
    "# @markdown Because the models are larger when using augmentation, the parameters are different.\n",
    "test_augmentations = True # @param {type: 'boolean'}\n",
    "_BASE_PATH = 'https://storage.googleapis.com/dm_compressed_vision/models/' # @param {type: 'string'}\n",
    "_SAVE_PATH = '/tmp/' # @param {type: 'string'}\n",
    "\n",
    "if not test_augmentations:\n",
    "  # @markdown Controllable params in non augmentation mode.\n",
    "  augmentation_type = None\n",
    "  compression_rate = 192 # @param {type: 'integer'}\n",
    "  if compression_rate == 192:\n",
    "    model_path = f'{_BASE_PATH}compression/41861759_1_cr%3D192.pkl'\n",
    "  if compression_rate == 236:\n",
    "    model_path = f'{_BASE_PATH}compression/42071788_1_cr%3D236.pkl'\n",
    "  if compression_rate == 384:\n",
    "    model_path = f'{_BASE_PATH}compression/41877908_2_cr%3D384.pkl'\n",
    "  if compression_rate == 384:\n",
    "    model_path = f'{_BASE_PATH}compression/41748435_4_cr%3D786.pkl'\n",
    "  NUM_FRAMES = 32 # @param\n",
    "  BATCH_SIZE = 4\n",
    "elif test_augmentations:\n",
    "  # @markdown Controllable params in augmentation mode.\n",
    "  augmentation_type = 'flip' # @param {type: 'string'} ['flip']\n",
    "  if augmentation_type == 'flip':\n",
    "    model_path = f'{_BASE_PATH}augmentations/35225852_7_augm%3Dflip.pkl'\n",
    "  else:\n",
    "    raise ValueError(f\"Unexpected augmentation {augmentation_type}.\")\n",
    "\n",
    "!wget $model_path -O '/tmp/model_path.pkl'\n",
    "\n",
    "with open('/tmp/model_path.pkl', 'rb') as f:\n",
    "  all_params = checkpoint_loader.load_params_state(f)\n",
    "\n",
    "augm_params = all_params['augm_params']\n",
    "augm_state = all_params['augm_state']\n",
    "augm_config = all_params['augm_config']\n",
    "params = all_params['params']\n",
    "state = all_params['state']\n",
    "config = all_params['config']\n",
    "\n",
    "if augm_config is not None:\n",
    "  augm_config = augm_config.experiment_kwargs.config\n",
    "  NUM_FRAMES = augm_config.data.num_frames\n",
    "  BATCH_SIZE = 1\n",
    "\n",
    "exp_config = config.experiment_kwargs.config\n",
    "exp_config.data.train_batch_size = BATCH_SIZE\n",
    "exp_config.data.eval_batch_size = BATCH_SIZE\n",
    "exp_config.data.num_frames = NUM_FRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9cgjNQK5sw9h"
   },
   "outputs": [],
   "source": [
    "# @title Load a video to test on.\n",
    "VIDEO_TO_TEST = f'https://storage.googleapis.com/dm_compressed_vision/data/video8.gif'\n",
    "!wget $VIDEO_TO_TEST -O '/tmp/video.gif'\n",
    "\n",
    "# Load video.\n",
    "with open('/tmp/video.gif', 'rb') as f:\n",
    "  sample_video = Image.open(f)\n",
    "  sample_video.seek(0)\n",
    "\n",
    "  images = []\n",
    "  try:\n",
    "      while True:\n",
    "          images.append(np.asarray(sample_video.convert()))\n",
    "          sample_video.seek(sample_video.tell()+1)\n",
    "  except EOFError:\n",
    "      pass\n",
    "print(f'Length of video: {len(images)} frames.')\n",
    "sample_video = np.array(images)[:NUM_FRAMES][None, :] / 255.\n",
    "v = video_utils.video_reshaper(sample_video)\n",
    "video_utils.save_video((v * 255).astype(np.uint8), '/tmp/display.gif')\n",
    "display_image(filename='/tmp/display.gif', embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "es-tRoLtsyha"
   },
   "outputs": [],
   "source": [
    "# @title Encode-decode functions.\n",
    "def forward_codec_fn():\n",
    "  codec_model = encoder_decoder_unet.CompressionConvEncoderDecoder(\n",
    "      num_channels=3,\n",
    "      **exp_config.model_kwargs,\n",
    "  )\n",
    "  return codec_model, {\n",
    "      'encoder': codec_model.encode,\n",
    "      'decoder': codec_model.decode,\n",
    "  }\n",
    "\n",
    "_, codec_apply_fns = (\n",
    "    hk.multi_transform_with_state(forward_codec_fn)\n",
    ")\n",
    "codec_encoder = codec_apply_fns['encoder']\n",
    "codec_decoder = codec_apply_fns['decoder']\n",
    "\n",
    "encode_decode_jitted = jax.jit(\n",
    "    lambda x: data_utils.encode_decode(\n",
    "        codec_encoder=codec_encoder,\n",
    "        codec_decoder=codec_decoder,\n",
    "        codec_params=params,\n",
    "        codec_state=state,\n",
    "        inputs=x,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZ3o5Nass0Kx"
   },
   "outputs": [],
   "source": [
    "# @title Run compression.\n",
    "decompressed_video, codes = encode_decode_jitted(sample_video)\n",
    "\n",
    "cpr = metric_utils.get_compression_rate(\n",
    "    sample_video,\n",
    "    codes,\n",
    "    bits_per_element=exp_config.model_kwargs.vq_num_embeddings,\n",
    ")\n",
    "print(f'Compression rate is {cpr}')\n",
    "print(f'Codes W x H is {codes[0].shape[2]} x {codes[0].shape[3]}')\n",
    "print(f'Codes channels is {codes[0].shape[-1]}')\n",
    "print(f'Codes time is {codes[0].shape[1]}')\n",
    "print(f'Decompressed space is {decompressed_video.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxmJqWals16R"
   },
   "outputs": [],
   "source": [
    "# @title Visualise reconstructed videos.\n",
    "e = video_utils.video_reshaper(decompressed_video)\n",
    "\n",
    "video_utils.save_video((e * 255).astype(np.uint8), '/tmp/display.gif')\n",
    "\n",
    "err = jnp.mean((sample_video - decompressed_video)**2)\n",
    "print(f'Mean L2-norm error is {err}')\n",
    "display_image(filename='/tmp/display.gif', embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CauME0eYs3Qx"
   },
   "outputs": [],
   "source": [
    "# @title Save video.\n",
    "ds_name = VIDEO_TO_TEST.split('.')[0]\n",
    "\n",
    "MAIN_PATH = f'{_SAVE_PATH}{ds_name}/neural_codec-unet/'\n",
    "if not os.path.exists(MAIN_PATH):\n",
    "  os.makedirs(MAIN_PATH)\n",
    "name = os.path.basename(model_path).split('.')[0]\n",
    "\n",
    "path = f'{MAIN_PATH}/compression-{name}-combined.gif'\n",
    "video_utils.save_video((e * 255).astype(np.uint8), path)\n",
    "\n",
    "MAIN_PATH = f'{_SAVE_PATH}{ds_name}/raw_videos/'\n",
    "if not os.path.exists(MAIN_PATH):\n",
    "  os.makedirs(MAIN_PATH)\n",
    "path = f'{MAIN_PATH}/original-{name}.gif'\n",
    "video_utils.save_video((v * 255).astype(np.uint8), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVlWFCpYtCJx"
   },
   "source": [
    "# Run augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_RIdpM0s4sJ"
   },
   "outputs": [],
   "source": [
    "# @title Setup\n",
    "\n",
    "if augmentation_type == 'flip':\n",
    "  def get_augmentation(video):\n",
    "    bs, _, _, _, _ = video.shape\n",
    "    yes_flip = jnp.ones(shape=(bs, 1, 1))\n",
    "    return yes_flip\n",
    "  num_frames = 32\n",
    "  pixel_width = 256\n",
    "else:\n",
    "  raise ValueError(\n",
    "      \"Augmentation type must be set above to run this part of the CoLAB.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUFY3kLKs62a"
   },
   "outputs": [],
   "source": [
    "# @title Functions\n",
    "\n",
    "def forward_codec_fn():\n",
    "  codec_model = encoder_decoder_unet.CompressionConvEncoderDecoder(\n",
    "      num_channels=3,\n",
    "      **exp_config.model_kwargs,\n",
    "  )\n",
    "  return codec_model, {\n",
    "      'encoder': codec_model.encode,\n",
    "      'decoder': codec_model.decode,\n",
    "  }\n",
    "\n",
    "@hk.transform_with_state\n",
    "def get_codes(inputs):\n",
    "  sample_video = inputs['image']\n",
    "  _, codec_apply_fns = (\n",
    "      hk.multi_transform_with_state(forward_codec_fn)\n",
    "  )\n",
    "  codec_encoder = codec_apply_fns['encoder']\n",
    "  codec_decoder = codec_apply_fns['decoder']\n",
    "\n",
    "  _, quantized = data_utils.convert_im_to_codes(\n",
    "      codec_encoder=codec_encoder,\n",
    "      codec_params=params,\n",
    "      codec_state=state,\n",
    "      images=sample_video,\n",
    "      is_return_quantized=True,\n",
    "  )\n",
    "\n",
    "  transformation = inputs['transformation']\n",
    "  equivariant_model = equivariant_networks.get_equivariant_network(\n",
    "      augm_config.augmentation.network)\n",
    "  quantized_original = quantized\n",
    "  quantized = quantized[:, None].repeat(transformation.shape[1], 1)\n",
    "  transformation = transformation.reshape((-1, transformation.shape[2]))\n",
    "  quantized = quantized.reshape((transformation.shape[0],) +\n",
    "                                quantized.shape[2:])\n",
    "  transform_quantized = equivariant_model(\n",
    "      **augm_config.augmentation.kwargs)(quantized, transformation)\n",
    "\n",
    "  transform_reconstruction = data_utils.convert_codes_to_im(\n",
    "      codec_decoder=codec_decoder,\n",
    "      codec_params=params,\n",
    "      codec_state=state,\n",
    "      codes=transform_quantized,\n",
    "      is_quantized=True,\n",
    "      )\n",
    "\n",
    "  return transform_reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRdQBsLZs8eC"
   },
   "outputs": [],
   "source": [
    "# @title Run augmentation\n",
    "def _bcast_local_devices(array):\n",
    "    array = jax.tree_map(\n",
    "        lambda a: a[None, :].repeat(jax.local_device_count(), 0), array)\n",
    "    return array\n",
    "\n",
    "pmapped_codes = jax.pmap(jax.jit(get_codes.apply))\n",
    "bcast_augm_params = _bcast_local_devices(augm_params)\n",
    "bcast_augm_state = _bcast_local_devices(augm_state)\n",
    "bcast_rng = _bcast_local_devices(jax.random.PRNGKey(0))\n",
    "sample_inputs = _bcast_local_devices(sample_video)\n",
    "\n",
    "transformation = jax.vmap(get_augmentation)(sample_inputs)\n",
    "\n",
    "inputs = {\n",
    "    'image': (\n",
    "        sample_inputs[:,:,:num_frames,:pixel_width, :pixel_width]), \n",
    "    'transformation': transformation\n",
    "}\n",
    "\n",
    "(transform_reconstruction), _ = pmapped_codes(\n",
    "    bcast_augm_params, bcast_augm_state, bcast_rng, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5kzvu7jEs-Ax"
   },
   "outputs": [],
   "source": [
    "# @title Visualise augmented videos with original video.\n",
    "video = np.concatenate((inputs['image'], transform_reconstruction), 1)\n",
    "v = video_utils.video_reshaper(video[0])\n",
    "\n",
    "video_utils.save_video((v * 255).astype(np.uint8), '/tmp/display.gif')\n",
    "display_image(filename='/tmp/display.gif', embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7pWYo0W6s_ep"
   },
   "outputs": [],
   "source": [
    "# @title Save augmented video.\n",
    "ds_name = VIDEO_TO_TEST.split('.')[0]\n",
    "MAIN_PATH = f'{_SAVE_PATH}{ds_name}/neural_codec-unet/augmented/'\n",
    "if not os.path.exists(MAIN_PATH):\n",
    "  os.makedirs(MAIN_PATH)\n",
    "\n",
    "path = f'{MAIN_PATH}/compression-{name}-combined.gif'\n",
    "video_utils.save_video((v * 255).astype(np.uint8), path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
